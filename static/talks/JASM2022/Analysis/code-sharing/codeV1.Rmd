---
title: "Anomaly detection in river networks"
author: "Puwasala Gamakumara"
date: "17/11/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = TRUE,
  fig.height = 6,
  fig.width = 7)

library(tidyverse)
library(lubridate)
library(mgcv)
```


# Data

We use Pringle Creek data from NEON [https://data.neonscience.org/data-products] 

```{r load}

load(here::here("code-sharing", "data", "PRIN_5min_cleaned.rda"))
load(here::here("code-sharing", "data", "PRIN_5min_flagged.rda"))
```

We choose data from 01-Oct-2019 to 31-Dec-2019

```{r preparing-data}
PRIN_5min_flagged <- PRIN_5min_flagged %>%
  rename(
    Timestamp = roundedTimestamp,
    level = surfacewaterElevMean,
    conductance = specificConductance,
    dissolved_oxygen = dissolvedOxygen,
    temperature = surfWaterTempMean
  ) %>%
  filter(Timestamp >= ymd("2019-10-01") & 
           Timestamp < ymd("2020-01-01")) %>% 
  mutate(Timestamp = ymd_hms(Timestamp))

PRIN_5min_cleaned <- PRIN_5min_cleaned %>%
  rename(
    Timestamp = roundedTimestamp,
    level = surfacewaterElevMean,
    conductance = specificConductance,
    dissolved_oxygen = dissolvedOxygen,
    temperature = surfWaterTempMean
  ) %>%
  filter(Timestamp >= ymd("2019-10-01") & 
           Timestamp < ymd("2020-01-01")) %>% 
  mutate(Timestamp = ymd_hms(Timestamp))


```

Visualising raw data 

```{r raw-data-plot}
PRIN_5min_flagged %>% 
  select(Timestamp, turbidity, conductance, dissolved_oxygen,
         level, temperature, site) %>% 
  pivot_longer(-c(Timestamp, site)) %>% 
  ggplot(aes(Timestamp, value, color = site)) +
  geom_line() +
  facet_wrap(~name, scales = "free_y", ncol = 1)
```

# Anomaly detection in downstream Turbidity

Pre-identified anomalies in Turbidity

```{r fig.height=4}

Turbidity_down <- PRIN_5min_flagged %>% 
  filter(site == "down") %>% 
  select(Timestamp, turbidity, turbidityAnomalyFlag) %>% 
  rename("turbidity_downstream" = turbidity)

Turbidity_down %>% 
  ggplot(aes(Timestamp, turbidity_downstream)) +
  geom_point(size = 0.5) +
  geom_point(data = Turbidity_down %>% 
               filter(turbidityAnomalyFlag == 1),
             aes(Timestamp, turbidity_downstream), 
             color = "red", 
             shape = 17,
             size = 2)
```

## Using single sensor data

Suppose we are using data from a single sensor. Then we model the downstream turbidity using other water-quality variables measured in the same sensor location. 


```{r downstream_scatter}

data_downstream <- PRIN_5min_flagged %>%
  filter(site == "down") %>% 
  select(Timestamp, turbidity, conductance, dissolved_oxygen,
         pH, level, temperature)
  
data_downstream %>% 
  select(-Timestamp) %>% 
  GGally::ggpairs()
    
    
```

We choose level, temperature, conductance and dissolved oxygen to model the downstream turbidity. We impute any missing values in the predictors prior to the analysis. I have used a Kalman filter based on ARIMA state-space model for imputation. The codes for this can be found here [https://github.com/PuwasalaG/ARCLP-workshop/blob/main/code-sharing/src/missing_value_imputation.R] 

First divide the data into training set (01-Oct-2019 to 30-Nov-2019) and test set (01-Dec-2019 to 31-Dec-2019). We use the training set to train the model. We remove any technical anomalies we identified as the model needs to be trained on a cleaned dataset. 

We fit the model on the log scale of turbidity as the data are very skewed.
```{r prepare-data-downstream}
# loading imputed data
load(here::here("code-sharing", "data", 
                "data_downstream_imputed.rda"))

# transformation and lag computation of the response
data_downstream_imputed <- data_downstream_imputed %>% 
  select(-turbidity_downstream) %>% 
  left_join(Turbidity_down) %>% 
  mutate(turbidity_downstream_log = log(turbidity_downstream),
         turbidity_downstream_log_lag1 = lag(turbidity_downstream_log, 1),
         turbidity_downstream_log_lag2 = lag(turbidity_downstream_log, 2),
         turbidity_downstream_log_lag3 = lag(turbidity_downstream_log, 3))

data_train <- data_downstream_imputed %>%
  filter(Timestamp >= ymd("2019-10-01")
         & Timestamp < ymd("2019-12-01"))


# We will remove the anomalies in the turbidity_downstream in training data
data_train <- data_train %>%
  mutate(turbidity_downstream = if_else(turbidityAnomalyFlag=="outlier",
                                        as.numeric(NA),
                                        turbidity_downstream))


```


```{r gam-down-AR}

gam_down_AR <- gam(turbidity_downstream_log ~
                     s(turbidity_downstream_log_lag1) +
                     s(conductance_downstream) +
                     s(dissolved_oxygen_downstream) +
                     s(level_downstream, k=6) +
                     s(temperature_downstream),
                   data = data_train)

summary(gam_down_AR)

visreg::visreg(gam_down_AR, plot = FALSE) %>% 
  purrr::map(function(x){plot(x, gg = TRUE) + theme_bw()}) %>% 
  gridExtra::marrangeGrob(ncol = 2, nrow = 3)

residuals <- gam_down_AR$residuals
forecast::ggtsdisplay(residuals, plot.type = "histogram")

```

We next get the predictions from the fitted model and compare that with the realisation. That is, we compute the residuals such that, 
$r_t = \text{log}(turbidity(down))_t - \hat{\text{log}(turbidity(down))}_t$ where $\hat{\text{log}(turbidity(down))}_t$ are the predictions from the fitted model. 

```{r get_resids_gam_down_AR}
data_residuals <- data_downstream_imputed%>%
  select(Timestamp, 
         turbidity_downstream, 
         turbidity_downstream_log,
         turbidityAnomalyFlag) %>%
  mutate(predict_mod_down_AR = as.numeric(predict.gam(gam_down_AR, 
                                                      newdata = data_downstream_imputed)),
         residuals_down_AR = turbidity_downstream_log - predict_mod_down_AR,
         turbidityAnomalyFlag = case_when(turbidityAnomalyFlag == 0 ~ "typical",
                                          turbidityAnomalyFlag == 1 ~ "outlier",
                                          TRUE ~ as.character(NA)))

```


Using SPOT to detect anomalies in the residuals. See the function for SPOT here [https://github.com/PuwasalaG/ARCLP-workshop/blob/main/code-sharing/src/SPOT_algorithm.R]

```{r SPOT-gam-down-AR}
source(here::here("code-sharing", "src", "SPOT_algorithm.R"))

q <- 0.01 # level of significance chosen for the particular application
t_prob <- 0.98 # probability for the percentile chosen for the threshold in POT
n = floor(nrow(data_residuals)*0.30) # first 30% of the data for calibrating initial outlier threshold (might contain missing values)

confusion_mod_down_AR <- compute_confusion(data = data_residuals,
                                           residuals = "residuals_down_AR",
                                           name = "model_down_AR", 
                                           n = n,
                                           q = q, 
                                           t_prob = t_prob,
                                           cmax = FALSE)

confusion_mod_down_AR <- confusion_mod_down_AR %>%
  rename(AnomalyFlag_SPOT_down_AR = AnomalyFlag_SPOT,
         confusion_mod_down_AR = confusion_matrix)
data_residuals <- data_residuals %>%
  left_join(confusion_mod_down_AR, by = "Timestamp")

```

```{r eval-gam-down-AR}
classif_plot_turb_down_AR <- data_residuals %>%
  select(Timestamp, turbidity_downstream, confusion_mod_down_AR) %>%
  mutate(confusion_mod_down_AR = factor(confusion_mod_down_AR,
                                        levels = c("TP", "TN", "FP", "FN",
                                                   "NA"))) %>%
  drop_na() %>%
  ggplot() +
  geom_point(aes(Timestamp, turbidity_downstream,
                 color = confusion_mod_down_AR,
                 shape = confusion_mod_down_AR,
                 size = confusion_mod_down_AR,
                 alpha = confusion_mod_down_AR)) +
  scale_color_manual(values = c("#009E73", "#999999", "#E69F00",
                                "#FF3333")) +
  scale_shape_manual(values = c(17,16,18,15)) +
  scale_size_manual(values = c(2,0.3,2,2)) +
  scale_alpha_manual(values = c(1,1,1,1)) +
  theme(legend.title = element_blank(), legend.position = "bottom") +
  ylab("Turbidity (FNU)")

calib_time <- data_residuals %>% 
  slice(1:n) %>% 
  pull(Timestamp)

TP_time <- data_residuals %>% 
  filter(confusion_mod_down_AR == "TP") %>% 
  pull(Timestamp)

TN_time <- data_residuals %>% 
  filter(confusion_mod_down_AR == "TN") %>% 
  pull(Timestamp)

FP_time <- data_residuals %>% 
  filter(confusion_mod_down_AR == "FP") %>% 
  pull(Timestamp)

FN_time <- data_residuals %>% 
  filter(confusion_mod_down_AR == "FN") %>% 
  pull(Timestamp)


classif_plot_resid_down_AR <- data_residuals %>%
  select(Timestamp, residuals_down_AR) %>%
  drop_na() %>%
  ggplot() +
  geom_point(data = data_residuals %>% 
               filter(Timestamp %in% calib_time),
             aes(Timestamp, residuals_down_AR), 
             color = "#000000",
             shape = 16,
             size = 0.3,
             alpha = 0.3) +
  geom_point(data = data_residuals %>% 
               filter(Timestamp > max(calib_time) &
                        Timestamp %in% TN_time),
             aes(Timestamp, residuals_down_AR),
             color = "#999999",
             shape = 16,
             size = 0.3,
             alpha = 0.3) +
  geom_point(data = data_residuals %>% 
               filter(Timestamp %in% TP_time),
             aes(Timestamp, residuals_down_AR),
             color = "#009E73",
             shape = 17,
             size = 2) +
  geom_point(data = data_residuals %>% 
               filter(Timestamp %in% FP_time),
             aes(Timestamp, residuals_down_AR),
             color = "#E69F00",
             shape = 18,
             size = 2) +
  geom_point(data = data_residuals %>% 
               filter(Timestamp %in% FN_time),
             aes(Timestamp, residuals_down_AR),
             color = "#FF3333",
             shape = 15,
             size = 2) +
  ylab("residuals")

library(patchwork)
classif_plot_turb_down_AR / classif_plot_resid_down_AR

```

This model correctly classifies all true technical anomalies. 
However, there are considerable amount of False Positives too. 



# Using multiple sensor data

Suppose we have a flow connected sensor upstream. That is water flows through upstream sensor to the downstream sensor. We can use data from that sensor to predict downstream turbidity series.

We first have to calculate the lag time between the two sensor locations (i.e. the time it takes water to flow from upstream to downstream sensor). We find this lag time by maximising the conditional cross-correlation between the two sensors.


## Lag time estimation

In this application we choose to use turbidity from upstream and downstream sensors to calculate the cross-correlations conditional on upstream water level and temperature. 

We first have to use a cleaned data set to compute the models. If necessary we also impute missing values. Please refer [https://github.com/PuwasalaG/ARCLP-workshop/blob/main/code-sharing/src/missing_value_imputation.R] to see missing value imputation of each series.


Install `conduits` package

```{r install-conduits}
# install.packages("devtools")
devtools::install_github("PuwasalaG/conduits")
```



```{r prep_data_lag_estim}
load(here::here("code-sharing", "data", 
                "data_downstream_imputed.rda"))
load(here::here("code-sharing", "data", 
                "data_upstream_imputed.rda"))


data_xz <- data_upstream_imputed %>% 
  select(Timestamp, level_upstream, temperature_upstream,
         turbidity_upstream)

data_y <- data_downstream_imputed %>% 
  select(Timestamp, turbidity_downstream)

data_lag_estim <- data_xz %>% 
  left_join(data_y, by = "Timestamp") %>% 
  mutate(turbidity_upstream = if_else(turbidity_upstream == 0, 
                                      as.numeric(NA), 
                                      turbidity_upstream))

```


Let $x_t:$ Turbidity upstream, $y_t:$ Turbidity downstream and $\bm{z}_t:$ {level upstream, temperature upstream}

$x^*_t = \frac{x_t - \text{E}[x_t|\bm{z}_t]}{\sqrt{\text{V}[x_t|\bm{z}_t]}}$ and
$y^*_t = \frac{y_t - \text{E}[y_t|\bm{z}_t]}{\sqrt{\text{V}[y_t|\bm{z}_t]}}$

$$
r_k(\bm{z}_t) = \text{E}[x_t^*y^*_{t+k}|\bm{z}_t] \quad \text{for} \quad k = 1,2,...
$$

To estimate $r_k(\bm{z}_t)$ we fit the following GAMs

Let $x_t^*y^*_{t+k}|\bm{z}_t \sim \text{N}(r_k(\bm{z}_t), \sigma_r^2)$,

$$
g(r_k(\bm{z}_t)) = \gamma_0 + \sum_{i=1}^p h_i(z_{i,t}) + \varepsilon_t
$$


$$
\hat{r}_k(\bm{z}_t) = g^{-1}( \hat{\gamma}_0 + \sum_{i=1}^p \hat{h}_i(z_{i,t}))
$$

### Estimating time delay

$$
\hat{d}_{t}(\bm{z}_t) = \underset{k}{\operatorname{argmax}}\quad \hat{r}_{k}(\bm{z}_t)
$$

```{r Conditional-cross-correlation}

# Setting degrees of freedom for smoothing parameters
mean <- c(5,5)
variance <- c(4,4)
correlation <- c(3,3)

# we choose level and temperature for lag time estimation in this analysis
?conduits::conditional_moments
?conduits::conditional_ccf

cond_ccf <- data_lag_estim %>%
  conduits::conditional_ccf(x = turbidity_upstream,
                            y = turbidity_downstream,
                            z_numeric = c(level_upstream, 
                                          temperature_upstream),
                            k = 1:24,
                            knots_mean = list(x = mean, 
                                              y = mean),
                            knots_variance = list(x = variance, 
                                                  y = variance),
                            df_correlation = correlation)

?autoplot.conditional_ccf

autoplot(cond_ccf, type = "mean")
autoplot(cond_ccf, type = "cross-correlation", k=2)
```

```{r estimate_dt}
?conduits::estimate_dt


Estimate_dt <- cond_ccf %>% 
  conduits::estimate_dt(new_data = data_lag_estim, k_min = 1, 
              k_max = 24)

Estimate_dt$data_dt

```

The last column of the *Estimate_dt$data_dt* gives the p-value for the estimated ccf.

```{r plot-ccf-dt-pval}
insig_times <- Estimate_dt$data_dt %>% 
  filter(pvalue > 0.01) %>% 
  pull(Timestamp)

df1 <- Estimate_dt$data_dt %>% 
  select(Timestamp, dt, ccf, pvalue) %>% 
  pivot_longer(-Timestamp) 
  

df1 %>% 
  mutate(name = factor(name, levels = c("dt", "ccf", "pvalue"))) %>% 
  ggplot(aes(Timestamp, value)) +
  geom_line() +
  geom_point(data = df1 %>% 
              filter(Timestamp %in% insig_times), 
            aes(Timestamp, value), color = "red", size = 0.3) +
  facet_wrap(~name, ncol = 1, scales = "free_y")

```

The red points in \ref{fig:plot-ccf-dt-pval} shows the insignificant ccfs. We can replace these with an appropriate value before computing lagged upstream variables. 



```{r vis_dt}
?autoplot.estimate_dt

# Setting interval==TRUE will compute the bootstrap confidence intervals. 
autoplot(object = Estimate_dt, m=1000, seed = 1989, 
                    interval = FALSE)
```

```{r Replacing-insignificant-lags}


# Finding the lag that gives maximum cross correlation 
# between between xt and y_t+k for k = 1,...,24

ccf_vec <- numeric()

for (i in 1:24) {
  ccf_vec[i] <- cor(data_xz$turbidity_upstream,
                   lead(data_y$turbidity_downstream, 
                        n = i), 
                   use = "complete.obs")
}

max_lag_ccf <- which.max(ccf_vec)

data_dt <- Estimate_dt$data_dt %>% 
  mutate(dt = if_else(pvalue > 0.01, as.numeric(max_lag_ccf), dt))

```

```{r map_lags}

data_dt <- data_dt %>% 
  select(Timestamp, turbidity_downstream, dt, ccf) 
data_upstream_new <- list(data_upstream_imputed, data_dt) %>% 
  reduce(left_join, by = "Timestamp")

?map_lags

data_upstream_new_dt <- data_upstream_new %>% 
  conduits::map_lags(up = c(level_upstream, conductance_upstream,
                  dissolved_oxygen_upstream,
                  turbidity_upstream, temperature_upstream,
                  pH_upstream), 
           down = c(turbidity_downstream), 
           lag = dt)


data_lagged_upstream <- data_upstream_new_dt %>% 
  select(-turbidity_downstream)

head(data_upstream_new_dt)
```

## Anomaly detection in downstream turbidity using upstream data

```{r}

# combining all data

data_all <- data_downstream_imputed %>% 
  select(-turbidity_downstream) %>% 
  list(Turbidity_down, data_lagged_upstream) %>% 
  purrr::reduce(left_join, by = "Timestamp")

data_all %>% 
  select(turbidity_downstream, turbidity_upstream_dt,
         conductance_upstream_dt, dissolved_oxygen_upstream_dt,
         level_upstream_dt, temperature_upstream_dt) %>% 
  rename("turbidity_down" = turbidity_downstream,
         "turbidity_up" = turbidity_upstream_dt,
         "conductance_up" = conductance_upstream_dt,
         "dissolved_oxygen_up" =
           dissolved_oxygen_upstream_dt,
         "level_up" = level_upstream_dt,
         "temperature_up" = temperature_upstream_dt) %>% 
  GGally::ggpairs()

```

We choose upstream water level, temperature, turbidity, conductance and dissolved oxygen to model downstream turbidity.

```{r prep_data_gam_up_AR}

## scaling-and-transformations

# computing log of turbidity
data_all <- data_all %>%
  mutate(turbidity_upstream_dt_log = log(turbidity_upstream_dt),
         turbidity_downstream_log = log(turbidity_downstream),
         turbidity_downstream_log_lag1 = lag(turbidity_downstream_log, 1),
         turbidity_downstream_log_lag2 = lag(turbidity_downstream_log, 2),
         turbidity_downstream_log_lag3 = lag(turbidity_downstream_log, 3))


## training-data
# taking data from "2019-10-01" to "2019-11-30" as training data
data_train <- data_all %>%
  filter(Timestamp >= ymd("2019-10-01")
         & Timestamp < ymd("2019-12-01"))


# We will remove the anomalies in the turbidity_downstream in training data
data_train <- data_train %>%
  mutate(turbidity_downstream = if_else(turbidityAnomalyFlag=="outlier",
                                        as.numeric(NA),
                                        turbidity_downstream))

```

```{r gam_up_AR}
gam_up_AR <- gam(turbidity_downstream_log ~
                   s(turbidity_downstream_log_lag1) +
                   s(turbidity_downstream_log_lag2) +
                   s(turbidity_downstream_log_lag3) +
                   s(turbidity_upstream_dt_log) +
                   s(level_upstream_dt) +
                   s(temperature_upstream_dt) +
                   s(conductance_upstream_dt),
                  data = data_train)

summary(gam_up_AR)

visreg::visreg(gam_up_AR, plot = FALSE) %>% 
  purrr::map(function(x){plot(x, gg = TRUE) + theme_bw()}) %>% 
  gridExtra::marrangeGrob(ncol = 2, nrow = 3)

residuals <- gam_up_AR$residuals
forecast::ggtsdisplay(residuals, plot.type = "histogram")

```

Computing residuals.

```{r resids_gam_up_AR}

data_residuals <- data_residuals %>%
  mutate(predict_mod_up_AR = as.numeric(predict.gam(gam_up_AR, 
                                                    newdata = data_all)),
         residuals_up_AR = turbidity_downstream_log - predict_mod_up_AR)

```

```{r SPOT-gam-up-AR}
#choosing the same hyper-parameters as GAM-up-AR residuals
confusion_mod_up_AR <- compute_confusion(data = data_residuals,
                                         residuals = "residuals_up_AR",
                                         name = "model_up_AR", n = n,
                                         q = q, t_prob = t_prob,
                                         cmax = FALSE)

confusion_mod_up_AR <- confusion_mod_up_AR %>%
  rename(AnomalyFlag_SPOT_up_AR = AnomalyFlag_SPOT,
         confusion_mod_up_AR = confusion_matrix)
data_residuals <- data_residuals %>%
  left_join(confusion_mod_up_AR, by = "Timestamp")

```

```{r eval-gam-up-AR}
## classification-GAM-up

classif_plot_turb_up_AR <- data_residuals %>%
  select(Timestamp, turbidity_downstream, confusion_mod_up_AR) %>%
  drop_na() %>%
  ggplot() +
  geom_point(aes(Timestamp, turbidity_downstream,
                 color = confusion_mod_up_AR,
                 shape = confusion_mod_up_AR,
                 size = confusion_mod_up_AR,
                 alpha = confusion_mod_up_AR)) +
  scale_color_manual(values = c("#009E73", "#999999", "#E69F00",
                                "#FF3333")) +
  scale_shape_manual(values = c(17,16,18,15)) +
  scale_size_manual(values = c(2,0.3,2,2)) +
  scale_alpha_manual(values = c(1,1,1,1)) +
  theme(legend.title = element_blank(), legend.position = "bottom") +
  ylab("Turbidity (FNU)")

calib_time <- data_residuals %>% 
  slice(1:n) %>% 
  pull(Timestamp)

TP_time <- data_residuals %>% 
  filter(confusion_mod_up_AR == "TP") %>% 
  pull(Timestamp)

TN_time <- data_residuals %>% 
  filter(confusion_mod_up_AR == "TN") %>% 
  pull(Timestamp)

FP_time <- data_residuals %>% 
  filter(confusion_mod_up_AR == "FP") %>% 
  pull(Timestamp)

FN_time <- data_residuals %>% 
  filter(confusion_mod_up_AR == "FN") %>% 
  pull(Timestamp)

classif_plot_resid_up_AR <- data_residuals %>%
  select(Timestamp, residuals_up_AR) %>%
  drop_na() %>%
  ggplot() +
  geom_point(data = data_residuals %>% 
               filter(Timestamp %in% calib_time),
             aes(Timestamp, residuals_up_AR), 
             color = "#000000",
             shape = 16,
             size = 0.3,
             alpha = 0.3) +
  geom_point(data = data_residuals %>% 
               filter(Timestamp > max(calib_time) &
                        Timestamp %in% TN_time),
             aes(Timestamp, residuals_up_AR),
             color = "#999999",
             shape = 16,
             size = 0.3,
             alpha = 0.3) +
  geom_point(data = data_residuals %>% 
               filter(Timestamp %in% TP_time),
             aes(Timestamp, residuals_up_AR),
             color = "#009E73",
             shape = 17,
             size = 2) +
  geom_point(data = data_residuals %>% 
               filter(Timestamp %in% FP_time),
             aes(Timestamp, residuals_up_AR),
             color = "#E69F00",
             shape = 18,
             size = 2) +
  geom_point(data = data_residuals %>% 
               filter(Timestamp %in% FN_time),
             aes(Timestamp, residuals_up_AR),
             color = "#FF3333",
             shape = 15,
             size = 2) +
  ylab("residuals")

classif_plot_turb_up_AR / classif_plot_resid_up_AR
```








